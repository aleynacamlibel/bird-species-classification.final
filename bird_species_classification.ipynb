{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a734c95",
   "metadata": {},
   "source": [
    "\n",
    "# Bird Species Classification - Deep Learning Project\n",
    "**Akbank Derin Öğrenme Bootcamp - Bird Species Dataset**\n",
    "\n",
    "Bu çalışmada Kaggle üzerindeki *Bird Species Classification* veri seti ile farklı kuş türlerini sınıflandırmayı deneyeceğiz. Adım adım ilerleyip hem kod hem de açıklamalar ekledim, böylece süreci takip etmek kolay olacak.\n",
    "\n",
    "**İçerik (kısa):**\n",
    "1. Proje amacı ve veri tanımı\n",
    "2. Gerekli paketlerin kurulumu ve ortam ayarları\n",
    "3. Veri indirme (Kaggle) ve klasör yapısı\n",
    "4. Keşifsel Veri Analizi (EDA) - örnek görselleştirmeler\n",
    "5. Veri ön işleme ve Data Augmentation\n",
    "6. Model mimarisi (Transfer Learning: EfficientNetB0) + alternatif basit CNN\n",
    "7. Eğitim, doğrulama, eğitim grafikleri\n",
    "8. Değerlendirme: Confusion Matrix, Classification Report\n",
    "9. Görselleştirme: Grad-CAM (model attention map)\n",
    "10. Basit Hiperparametre arama (örnek Keras Tuner kullanımı)\n",
    "11. Sonuçlar ve README için özet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2a2d7",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Proje Tanıtımı\n",
    "\n",
    "Bu projede amaç, kuş türlerini doğru şekilde sınıflandırabilecek bir derin öğrenme modeli geliştirmektir.  \n",
    "Kullanılacak veri seti Kaggle'da yer alan **Bird Species Classification** veri setidir.  \n",
    "\n",
    "Bu bölümde kısaca şunlardan bahsediyoruz:\n",
    "- Projenin hedefi\n",
    "- Kullanılacak veri setinin temel özellikleri\n",
    "- Çalışmada izlenecek yol haritası\n",
    "\n",
    "Sonraki adımlarda ortam hazırlığı, veri ön işleme, model kurulumu, eğitim ve değerlendirme aşamaları adım adım gösterilecektir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1e612",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Ortam Hazırlığı ve Gereksinimler\n",
    "\n",
    "Aşağıdaki hücreler ortam kurulumunu (gerekirse) yapar. Kaggle'dan veri indirmeniz için `kaggle` paketi ve API token'ına ihtiyaç vardır. Eğer Kaggle kullanmak istemezseniz, veri setini kendiniz indirip notebook çalışma dizinine yükleyebilirsiniz.\n",
    "\n",
    "> **Not:** Bu notebook Kaggle üzerinde çalışacak şekilde tasarlanmıştır. Localde çalıştırırken Kaggle API token (`kaggle.json`) veya manuel indirme yoluyla veri yüklenmelidir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcee501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gerekli kütüphaneleri yüklüyoruz (Kaggle ortamında çoğu zaten mevcut)\n",
    "# Eğer Kaggle ortamında çalışıyorsanız bu hücreyi atlayabilirsiniz.\n",
    "!pip install -q kaggle tensorflow matplotlib scikit-learn pandas seaborn opencv-python-headless tensorflow-addons\n",
    "!pip install -q -U keras-tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b62f2e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Veri Setini İndirme\n",
    "\n",
    "Aşağıdaki adımlar Kaggle API ile veri indirme içindir:\n",
    "1. Kaggle hesabınızda **Account > API** kısmından `kaggle.json` dosyasını indirin.\n",
    "2. Notebook ortamınıza `kaggle.json` dosyasını yükleyin (Kaggle'da dosyayı doğrudan yüklediyseniz bu adımı atlayın).\n",
    "3. Aşağıdaki hücreyi çalıştırarak veri setini indirin.\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/datasets/akash2907/bird-species-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kaggle API üzerinden veri indirme (çalışması için kaggle.json yüklenmiş olmalı)\n",
    "import os\n",
    "if not os.path.exists('kaggle.json'):\n",
    "    print('kaggle.json bulunamadı - eğer Kaggle üzerinde çalışıyorsanız bu uyarıyı göz ardı edin.')\n",
    "else:\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "    !kaggle datasets download -d akash2907/bird-species-classification -p ./ --unzip\n",
    "    print('İndirme tamamlandı.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbe57f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Veri Yapısı ve Keşifsel Analiz (EDA)\n",
    "\n",
    "Veri setini indirdikten sonra klasör yapısını kontrol edin. Aşağıda örnek olarak `train/` içinde sınıf klasörleri olduğu varsayılıyor. Hedefimiz resimleri `ImageDataGenerator` veya `tf.keras.utils.image_dataset_from_directory` ile yüklemektir.\n",
    "\n",
    "Ayrıca sınıf dağılımını görselleştirerek dengesizlik olup olmadığını kontrol edin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Klasör yapısını kontrol edelim\n",
    "import os, pathlib\n",
    "base_dir = './train'  # eğer farklı ise burayı güncelleyin\n",
    "if os.path.exists(base_dir):\n",
    "    classes = [d.name for d in pathlib.Path(base_dir).iterdir() if d.is_dir()]\n",
    "    print(f'{len(classes)} sınıf bulundu. İlk 10 sınıf örneği:', classes[:10])\n",
    "else:\n",
    "    print(f'{base_dir} bulunamadı. Lütfen veri dizinini kontrol edin.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sınıf dağılımını görselleştiriyoruz\n",
    "import matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "if os.path.exists(base_dir):\n",
    "    counts = {c: len(list(pathlib.Path(base_dir).joinpath(c).glob('*.jpg'))) for c in classes}\n",
    "    df_counts = pd.DataFrame.from_dict(counts, orient='index', columns=['count']).sort_values('count', ascending=False)\n",
    "    display(df_counts.head(20))\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=df_counts.index[:20], y='count', data=df_counts.reset_index().rename(columns={'index':'class'}))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Top 20 sınıf örnek sayısı')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Veri bulunamadığı için EDA atlandı.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3377948",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Veri Ön İşleme ve Data Augmentation\n",
    "\n",
    "Bu bölümde `tf.keras.preprocessing.image.ImageDataGenerator` veya `tf.data` API kullanacağız. Örnek olarak `image_dataset_from_directory` (tf.data tabanlı) gösterilecektir. Ayrıca eğitim sırasında augmentasyon uygulanacaktır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b05615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "IMAGE_SIZE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def get_datasets(train_dir, val_dir, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        label_mode='categorical',\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        label_mode='categorical',\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    class_names = train_ds.class_names\n",
    "    return train_ds, val_ds, class_names\n",
    "\n",
    "print('Dataset pipeline hazır - veri dosyalarınız mevcutsa train/ ve val/ yollarını vererek kullanabilirsiniz.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e88237",
   "metadata": {},
   "source": [
    "\n",
    "### Data Augmentation (tf.keras.layers üzerinden)\n",
    "Burada `tf.keras.Sequential` içinde augmentasyon layer'ları kullanacağız. Bu GPU üzerinde verimli çalışır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.12),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name='data_augmentation')\n",
    "\n",
    "# Augmentation örneklerini görselleştirelim\n",
    "def visualize_augmentation(dataset, n=3):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for images, labels in dataset.take(1):\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i in range(n):\n",
    "            ax = plt.subplot(1,n,i+1)\n",
    "            aug = data_augmentation(tf.expand_dims(images[i], 0))\n",
    "            plt.imshow(aug[0].numpy().astype('uint8'))\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print('Augmentation layer hazır.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5557c",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model - Transfer Learning örneği (EfficientNetB0)\n",
    "\n",
    "Transfer learning kullanarak önceden eğitilmiş bir ağdan faydalanacağız. Bu örnek `tf.keras.applications.EfficientNetB0` kullanır. Son katmanlarda dropout ve dense katmanları ile sınıflandırma yapacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(num_classes, image_size=IMAGE_SIZE, lr=1e-4, freeze_base=True):\n",
    "    inputs = Input(shape=(image_size[0], image_size[1], 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    base = EfficientNetB0(include_top=False, input_tensor=x, weights='imagenet')\n",
    "    if freeze_base:\n",
    "        base.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model oluşturma fonksiyonu hazır.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0387fb",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Modelin Eğitilmesi\n",
    "\n",
    "Aşağıdaki hücre, modelin nasıl eğitileceğini gösterir. Callbacks: ModelCheckpoint, ReduceLROnPlateau, EarlyStopping kullanacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Eğitim ve doğrulama veri dizinlerini buraya girmeliyiz\n",
    "train_dir = './train'\n",
    "val_dir = './valid'  # ya da ayrı val dizini yoksa farklı bir yol kullanın\n",
    "\n",
    "# Eğer veriler hazırsa dataset pipeline'ını çalıştıralım\n",
    "if os.path.exists(train_dir) and os.path.exists(val_dir):\n",
    "    train_ds, val_ds, class_names = get_datasets(train_dir, val_dir)\n",
    "    num_classes = len(class_names)\n",
    "    model = build_model(num_classes=num_classes, freeze_base=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[checkpoint, early, reduce_lr])\n",
    "else:\n",
    "    print('Eğitim atlandı - lütfen train_dir ve val_dir yollarını kontrol edin.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10218c6f",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Modelin Değerlendirilmesi\n",
    "\n",
    "Modelin eğitim/validasyon loss & accuracy grafikleri ile değerlendirmesini yapın. Ayrıca test setiniz varsa test üzerinde performansı hesaplayın.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb293e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend(); plt.title('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['accuracy'], label='train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.legend(); plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "if 'history' in globals():\n",
    "    plot_history(history)\n",
    "else:\n",
    "    print('Eğitim sonucu bulunamadı - history değişkeni mevcut değil.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d1dcb",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion Matrix & Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e11d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "if os.path.exists('best_model.h5') and os.path.exists(val_dir):\n",
    "    model.load_weights('best_model.h5')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels_batch in val_ds:\n",
    "        preds = model.predict(images)\n",
    "        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=False, fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Confusion matrix hesaplaması atlandı. Model veya validation set bulunamadı.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b365309",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Grad-CAM ile Modelin Nereye Baktığını Görselleştirme\n",
    "\n",
    "Aşağıdaki fonksiyon Grad-CAM hesaplayıp test görüntüleri üzerinde modelin hangi bölgelere dikkat ettiğini gösterir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e77dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def get_gradcam_heatmap(model, img_array, last_conv_layer_name):\n",
    "    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_gradcam(img_path, model, last_conv_layer_name='top_conv'):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMAGE_SIZE)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_arr = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    heatmap = get_gradcam_heatmap(model, input_arr, last_conv_layer_name)\n",
    "    heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img_array\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1); plt.imshow(img_array.astype('uint8')); plt.axis('off'); plt.title('Original')\n",
    "    plt.subplot(1,2,2); plt.imshow(superimposed_img.astype('uint8')); plt.axis('off'); plt.title('Grad-CAM')\n",
    "    plt.show()\n",
    "\n",
    "print('Grad-CAM fonksiyonları hazır.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee222cb9",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Hiperparametre Optimizasyonu (Keras Tuner ile Deneme)\n",
    "\n",
    "Basit bir Keras Tuner örneği gösterilmektedir. Bu hücre isteğe bağlıdır ve Kaggle ortamında çalıştırmadan önce kaynak ve zaman gereksinimlerini göz önünde bulundurun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd49d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def build_model_hp(hp):\n",
    "    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    base = EfficientNetB0(include_top=False, input_tensor=x, weights='imagenet')\n",
    "    base.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "    units = hp.Choice('units', [128, 256, 512])\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    outputs = Dense(len(class_names), activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(hp.Choice('lr', [1e-3, 1e-4, 1e-5])), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Örnek tuner (yorum satırı)\n",
    "# tuner = kt.RandomSearch(build_model_hp, objective='val_accuracy', max_trials=5, directory='tuner_dir', project_name='bird_tuning')\n",
    "# tuner.search(train_ds, validation_data=val_ds, epochs=5)\n",
    "print('Keras Tuner örneği hazır (yorum satırı halinde).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ce448",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Genel Değerlendirme ve README İçin Notlar\n",
    "\n",
    "\n",
    "\n",
    "- **Projenin amacı:** Bird species sınıflandırma.\n",
    "- **Veri seti:** Kaggle `akash2907/bird-species-classification` - ~9.000 görüntü, 15 sınıf (not: gerçek sayı değişebilir).\n",
    "- **Yöntem:** Transfer learning (EfficientNetB0), data augmentation, Grad-CAM analizi.\n",
    "- **Elde edilen sonuçlar:** (Eğitim sonrası en iyi doğruluk, loss, confusion matrix özetleri)\n",
    "- **Hiperparametre deneyleri:** ( learning rate, dropout, batch size)\n",
    "- **Çalıştırma talimatları:** Kaggle'da çalıştırmak için `kaggle.json` yüklenmeli, notebook hücreleri sırasıyla çalıştırılmalıdır.\n",
    "- **Kaggle Notebook linki:** (https://www.kaggle.com/aleynacamlibel)\n",
    "\n",
    "---\n",
    "\n",
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
